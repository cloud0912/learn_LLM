1. 看了看DEEPSEEK中的MLA操作
	——将KVcahe中的维度降低来降低显存占用

2. 了解了Flashattention：
	——传统的attention中的操作在长序列中会出现attention维度多大导致无法将其存储在SRAM中，
只能放在HBM中。
	——主要贡献在于io读写的优化，使得模型实际训练速度增快，而非专注于优化理论模型的flops

3. 发现了一个github挺好：
	——https://github.com/luhengshiwo/LLMForEverybody/