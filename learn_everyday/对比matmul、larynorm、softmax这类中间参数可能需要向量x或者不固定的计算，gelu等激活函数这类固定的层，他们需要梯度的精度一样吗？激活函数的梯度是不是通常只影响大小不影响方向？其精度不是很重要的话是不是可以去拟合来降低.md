对比matmul、larynorm、softmax这类中间参数可能需要向量x或者不固定的计算，gelu等激活函数这类固定的层，他们需要梯度的精度一样吗？激活函数的梯度是不是通常只影响大小不影响方向？其精度不是很重要的话是不是可以去拟合来降低显存，优化器对其影响吗?