---

## 大模型微调太贵？带你读懂参数高效微调 (PEFT) 的技术路线与方法

今天，我们来聊一个在大模型时代非常火热，而且对于想要入门或者深入了解LLM（Large Language Model）的朋友们来说至关重要的技术领域——**参数高效微调（Parameter-Efficient Fine-Tuning, PEFT）**。

你可能听说过GPT-3、LLaMA、ChatGLM这些庞然大物，它们拥有几百亿甚至上千亿的参数，能力惊人。但随之而来的问题是：**如果我们想让这些通用大模型在某个特定任务（比如医疗问答、法律文书生成、或者你的公司内部知识库）上表现得更好，怎么办？**

传统的做法是**全量微调（Full Fine-tuning）**，就像给一个全科医生进行专科培训，需要调整他所有的知识（参数）。但对于LLM来说，这意味着：

1.  **计算资源消耗巨大**：你需要非常强大的GPU集群，训练时间和成本高昂。
2.  **存储成本高**：每个任务微调后，你都会得到一个和原始模型差不多大小的新模型副本。如果你有100个任务，就得存100个巨大的模型！
3.  **容易灾难性遗忘（Catastrophic Forgetting）**：在学习新知识时，模型可能会忘记之前学到的通用知识。

这显然对于大多数开发者和中小型企业来说是难以承受的。于是，**PEFT技术应运而生，它的核心思想就是：能不能只动一小部分“开关”，就能让大模型适应新任务？**

**就像你请一位经验丰富的专家（预训练大模型），你不需要重新教他所有基础知识，只需要给他一些特定领域的指导手册或者让他带几个“实习生”（少量新增参数），他就能快速在该领域发挥专长。**

### PEFT的核心目标与优势

PEFT的目标是在保持预训练模型大部分参数不变的前提下，通过微调少量参数或增加少量额外参数，来适应下游任务。

它的主要优势显而易见：

*   **低成本**：显著减少计算资源和时间需求。
*   **少存储**：只需要存储少量修改或增加的参数，大大降低存储压力。
*   **易部署**：可以方便地为多个任务部署不同的“轻量级”适配器。
*   **缓解遗忘**：由于大部分基础模型参数被冻结，能更好地保留通用能力。

### PEFT的主要技术路线和方法

经过几年的发展，PEFT形成了几个主要的技术流派，我们可以大致归纳为以下几类：

#### 1. 添加附加参数（Additive Methods）

这类方法的核心思想是**冻结原始模型的绝大部分参数，然后添加一些新的、参数量较小的模块或参数**，在训练时只更新这些新添加的部分。

*   **Adapter Tuning (适配器微调)**：
    *   **原理**：这是比较早期的PEFT方法。它在Transformer的每个层（或部分层）中插入两个小的、瓶颈结构（bottleneck）的前馈神经网络模块（即Adapter模块）。通常是先降维，经过一个非线性激活函数，再升维恢复到原来的维度。
    *   **形象比喻**：就像在预训练模型的“信息高速公路”的每个收费站旁边，加了一个小小的“咨询台”。信息流过时，会先去咨询台“问问路”（经过Adapter模块处理），然后再继续前进。训练时，我们只训练这些咨询台的工作人员。
    *   **优点**：模型结构清晰，易于实现。
    *   **缺点**：可能会增加一点点推理时的延迟，因为信息流需要额外经过这些小模块。

*   **Prefix-Tuning / Prompt-Tuning / P-Tuning系列**：
    *   **原理**：这类方法受到人类如何通过提示（Prompt）引导模型生成内容的启发。它们不是修改模型内部结构，而是在输入层或者模型的注意力层**添加一些可学习的连续向量（称为Virtual Tokens或Prefixes）**。模型在处理任务时，会把这些可学习的向量和正常的输入一起处理，从而引导模型的行为。
        *   **Prompt-Tuning**：只在输入嵌入层添加可学习的Tokens。
        *   **Prefix-Tuning**：在每一层的Key和Value向量前添加可学习的Prefixes。
        *   **P-Tuning**：使用一个小的LSTM或MLP来生成这些可学习的Tokens，使其更具上下文关联性。P-Tuning v2则进一步优化，应用到更深层。
    *   **形象比喻**：想象你在给大模型下达指令。Prefix/Prompt Tuning就像是在你的指令前面加上一句“魔法咒语”（可学习的向量），这句咒语能引导模型更好地理解和执行你的特定任务，而我们只需要学习这句“咒语”怎么念。
    *   **优点**：需要更新的参数量极少（有时只有几十万甚至几万），效果也不错。
    *   **缺点**：训练有时不如Adapter或LoRA稳定，性能上限可能稍低（尤其早期版本）。

#### 2. 选择性微调（Selective Methods）

这类方法**选择性地只微调模型中的一部分已有参数**，比如只微调Bias项（偏置参数），或者只微调顶部的几层。

*   **原理**：基于假设——模型的部分参数对适应新任务更重要。例如，Bias参数通常被认为与模型的风格或领域适应性关联更强。
*   **现状**：虽然思路直接，但在实践中，其效果和普适性往往不如Additive或Reparameterization方法，目前相对不是主流的PEFT方向，但其思想仍有借鉴意义。
*   **BitFit**：仅调整模型中的偏置项（Bias Terms），其他参数全部冻结。实验表明，BERT-large仅需调整0.1%的参数即可在GLUE任务中达到95%的全量微调性能[14](https://blog.csdn.net/qq_41458274/article/details/141383353)。
*   **DiffPruning**：通过稀疏化方法动态选择需微调的参数子集，减少计算冗余。

#### 3. 基于重参数化（Reparameterization-based Methods）

这类方法通过引入低秩分解等技术来“间接”地修改模型的参数。

*   **LoRA (Low-Rank Adaptation)**：
    *   **原理**：这是目前**最流行、应用最广泛**的PEFT方法之一。LoRA的核心假设是：模型在适应新任务时，其参数矩阵的变化（ΔW）是低秩的。也就是说，这个变化可以用两个更小的矩阵（A和B）的乘积来近似表示 (ΔW ≈ BA)。在微调时，原始权重W保持冻结，我们只训练这两个小的低秩矩阵A和B。推理时，可以将学习到的BA加回到原始权重W上（W' = W + BA），这样不会引入额外的推理延迟。
    *   **形象比喻**：想象原始模型的参数矩阵W是一张巨大的、复杂的高清地图。全量微调是重新绘制整张地图。LoRA则是认为，针对新任务的“路线更新”，只需要在原地图上贴几张小的“修正贴纸”（BA），而我们只需要学习制作这些贴纸（训练A和B）。贴纸很小，制作起来很快。用的时候，可以直接把贴纸固定在地图上。
    *   **优点**：效果通常很好，训练效率高，推理时可以合并参数（无额外延迟），实现简单。
    *   **缺点**：秩（Rank）的选择是一个超参数，需要调整。

*   **QLoRA (Quantized LoRA)**：
    *   **原理**：LoRA的进一步优化。它在LoRA的基础上，结合了**模型量化（Quantization）**技术。在训练时，将冻结的预训练模型参数量化到更低的数据类型（比如4-bit），大大减少了训练时的显存占用。同时，LoRA部分仍然用较高精度（如16-bit）进行训练，保证了微调的精度。
    *   **优点**：**极大降低了显存门槛**，使得在消费级GPU上微调非常大的模型成为可能。
    *   **缺点**：实现相对复杂一些，需要处理量化和反量化的细节。

### PEFT的发展路径与趋势

回顾PEFT的发展，我们可以看到一条清晰的脉络：

1.  **早期探索（~2019-2020）**：以Adapter为代表，验证了“只动一小部分”的可行性。
2.  **Prompt范式兴起（~2021）**：Prefix-Tuning、Prompt-Tuning等方法展示了通过“外部引导”进行高效微调的可能性，参数量进一步降低。
3.  **LoRA异军突起（~2021-至今）**：LoRA凭借其出色的效果、效率和易用性，迅速成为主流，并衍生出QLoRA等众多改进版本，极大地推动了PEFT技术的普及。
4.  **融合与未来**：目前的研究趋势包括：
    *   **组合方法**：尝试结合不同PEFT方法的优点。
    *   **自动化选择**：研究如何自动确定哪些参数最值得微调。
    *   **更极致的效率**：探索在更低资源下实现高性能微调的方法。
    *   **理论理解**：深入探究PEFT为何有效，以及不同方法背后的数学原理。

### 如何准备面试中的PEFT问题？

如果你在面试中被问到PEFT，面试官可能想了解：

1.  **你知道为什么要用PEFT吗？** （回答：解决全量微调的资源消耗、存储、部署、遗忘问题）
2.  **你知道哪些主流的PEFT方法？** （至少能说出Adapter, Prompt-Tuning/Prefix-Tuning, LoRA）
3.  **能简单解释一下LoRA的原理吗？** （回答：冻结原模型，用低秩矩阵分解近似参数变化，只训练低秩矩阵A和B，推理时可合并）
4.  **LoRA相比Adapter有什么优势？** （通常效果更好，推理时可合并无延迟）
5.  **你知道QLoRA吗？它解决了什么问题？** （回答：结合量化，极大降低显存占用）
6.  **PEFT方法之间有哪些主要的trade-off？** （性能 vs. 参数量 vs. 训练/推理效率 vs. 实现复杂度）

**关键是展现你对PEFT动机的理解、对核心方法的掌握以及对它们之间差异和优劣的认知。**

### 总结

PEFT技术是推动大模型落地应用的关键赋能者。它就像一套精巧的“杠杆”，让我们能以较小的代价撬动强大的预训练模型，使其服务于各种具体场景。从Adapter到Prompt-Tuning，再到如今大放异彩的LoRA及其变种，PEFT的技术路线不断演进，目标始终是追求更高的效率和更好的性能。

理解PEFT不仅能帮助你跟上大模型技术的前沿，更能让你在实际项目中做出更合适的选型，甚至在求职面试中脱颖而出。希望这篇博客能为你打开一扇了解PEFT的大门！



参考链接：

https://blog.csdn.net/qq_41458274/article/details/141383353