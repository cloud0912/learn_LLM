这是篇文章旨在帮助初学者理解大模型全量微调的显存需求，并引出参数高效微调（PEFT）的概念。

---

## 【LLM】训练一个6B大模型要多少显存？全量微调 vs 参数高效微调

最近，LLM 的热潮席卷了整个技术圈，很多同学都对这些强大的模型充满了好奇，甚至跃跃欲试，想要亲自“调教”一番。

在实践中，我们经常遇到的第一个“拦路虎”就是：**显存**。很多同学可能会问：“我听说现在动不动就是几十亿、上百亿参数的模型，想要在我自己的机器上微调一下，比如一个 60 亿（6B）参数的模型，到底需要多少显存呢？”

这是一个非常好的问题！理解显存占用，不仅能帮助我们选择合适的硬件，更能让我们明白为什么**参数高效微调（Parameter-Efficient Fine-Tuning, PEFT）**技术如此重要。

今天，我们就以一个 6B 参数的大模型为例，庖丁解牛般分析一下**全量微调（Full Fine-tuning）**所需的显存，并自然地引出 PEFT 这个强大的“省钱”利器。

### 全量微调：为什么需要这么多显存？

全量微调，顾名思义，就是加载预训练好的模型权重，然后在我们自己的特定任务数据上，更新模型的所有参数。听起来很简单，但魔鬼藏在细节里。在训练过程中，显存主要被以下几个部分占用：

1.  **模型参数（Model Parameters）**
2.  **梯度（Gradients）**
3.  **优化器状态（Optimizer States）**
4.  **激活值（Activations）**
5.  **临时缓冲区和输入数据（Temporary Buffers & Input Data）**

让我们逐一分析，并估算一个 6B 模型的显存占用。

#### 1. 模型参数

这是模型本身的大小。参数量是 60 亿（6B）。存储这些参数需要多少显存取决于我们使用的**精度**：

*   **FP32 (单精度浮点数):** 每个参数需要 4 个字节。
    *   显存占用: 6B * 4 bytes = 24 GB
*   **FP16 (半精度浮点数) / BF16 (bfloat16):** 每个参数需要 2 个字节。这是目前训练和微调 LLM 最常用的精度，可以在保持模型性能的同时显著减少显存占用和计算量。
    *   显存占用: 6B * 2 bytes = 12 GB
*   **INT8 (8位整数):** 每个参数需要 1 个字节。通常用于推理加速，训练中较少直接使用（除非是特定的量化训练技术）。
    *   显存占用: 6B * 1 byte = 6 GB

**假设我们使用最常见的 FP16/BF16 精度进行微调，模型参数本身就需要 12 GB 显存。**

#### 2. 梯度

在反向传播过程中，我们需要为模型的**每一个**可训练参数计算梯度。梯度的数量和模型参数量相同。通常，梯度的精度也与模型参数的精度一致。

*   **FP32 梯度:**  6 * 10^9 * 4 bytes = **24 GB**

*   **FP16/BF16 梯度:**
    *   显存占用: 6B * 2 bytes = 12 GB

**梯度又需要额外的 12 GB 显存。**

#### 3. 优化器状态

为了更新模型参数，我们需要使用优化器，例如 Adam 或 AdamW。这些优化器通常需要存储一些**中间状态**，以便更有效地更新参数。

*   **Adam/AdamW:** 这是最常用的优化器之一。它需要为每个模型参数存储两个状态：
    
    *   **动量（Momentum）:** 通常与参数或梯度精度相同。
    *   **方差（Variance）:** 通常与参数或梯度精度相同。
    
    然而，为了训练的稳定性，即使模型参数和梯度使用 FP16，优化器的状态**通常会保持在 FP32**（单精度）。这是一种常见的混合精度训练策略。
    
*   **FP32 优化器状态:**
    *   动量 (Momentum): 6B * 4 bytes = 24 GB
    *   方差 (Variance): 6B * 4 bytes = 24 GB
    *   总优化器状态显存: 24 GB + 24 GB = 48 GB

**是的，你没看错！仅仅是 AdamW 优化器的状态，可能就需要高达 48 GB 的显存！** （*注意：有些实现或配置下，优化器状态也可以用 FP16 存储，这样会减少到 24GB，但这可能会牺牲一些训练稳定性或收敛效果*）。

#### 4. 激活值

这是训练过程中最“狡猾”也最难精确估算的显存占用部分。在前向传播过程中，每一层的输出（激活值）都需要被存储下来，以便在反向传播时计算梯度。

这是在训练期间**最难精确估算且变化最大**的部分。它取决于：

*   **批量大小 (Batch Size - B):** 每个 GPU 处理的样本数。
*   **序列长度 (Sequence Length - S):** 输入序列的最大 Token 数。
*   **模型隐藏层维度 (Hidden Dimension - H)**
*   **模型层数 (Number of Layers - L)**
*   **模型架构细节 (Attention机制等)**
    *   **影响：** 复杂且依赖具体实现。
    *   **解释：** 不同的架构或同一架构的不同实现方式会影响需要存储哪些具体的中间值以及它们的精确大小。
        *   **自注意力 (Self-Attention):** 需要存储 Q, K, V 矩阵，注意力输出等。如前所述，注意力得分矩阵可能是个巨大的临时存储。
        *   **前馈网络 (Feed-Forward Network - FFN):** 中间扩展层的激活值是主要的显存消耗者。
        *   **归一化层 (Normalization Layers):** 如 LayerNorm，也需要存储其计算过程中的中间统计量（均值、方差）或输入，以便反向传播。
        *   **残差连接 (Residual Connections):** 需要存储添加残差之前的那个分支的输出。
        *   **其他复杂结构：** 如 MoE (Mixture of Experts) 等会引入更复杂的激活值存储模式。
*   **是否使用梯度检查点 (Gradient Checkpointing / Activation Recomputation)**

激活值是在前向传播过程中计算出来的中间结果，需要暂存下来供反向传播时计算梯度。其大小约等于 `B * S * H * L * c * (激活值存储精度对应的字节数)`。

*   对于一个 6B 参数的 Transformer 模型（类似 Llama 架构），H 可能在 4096 左右，L 在 32 层左右。
*   假设使用 **BF16/FP16** (2 bytes) 存储激活值。
*   假设一个**适中的 Batch Size (B=4) 和 Sequence Length (S=2048)**。
*   `C` 是一个常数因子，代表**平均每层每个 Token 每个隐藏单元需要存储的激活值数量**。这个因子非常依赖架构。对于 Transformer，它不仅包括层输出 `(B, S, H)`，还包括 FFN 的中间扩展层 `(B, S, intermediate_size)`，以及 Q, K, V 等。`C` 可能在 10 到 24 甚至更高，取决于具体实现和计算方式。

一个非常粗略的经验法则是，激活值内存可能与模型参数、梯度、优化器状态的总和处于相似的数量级，甚至更大。

*   **无梯度检查点:** 激活值内存可能非常巨大。对于 B=4, S=2048, H=4096, L=32, BF16 的配置，激活值可能轻松达到 **几十 GB甚至上百 GB**。例如，粗略估算可能在 **40 GB - 100 GB** 之间，具体取决于实现细节和模型结构。
*   **使用梯度检查点:** 这项技术通过在反向传播时重新计算部分激活值，而不是全部存储，来**显著减少显存占用**，但会增加计算时间（约 20-30%）。使用梯度检查点后，激活值内存占用可以**大幅降低**，可能降至 **5 GB - 20 GB** 的范围，具体取决于检查点的设置策略。

#### 5. 临时缓冲区和输入数据

这部分包括：

*   输入数据本身（Token IDs, Attention Masks 等）转换成 Embedding 后的显存占用。
*   CUDA Kernel 运行时可能需要的临时显存。
*   各种计算过程中产生的中间变量。

这部分通常相比前面几项要小，但也不能完全忽略，尤其是在显存非常紧张的情况下。

#### 总结一下：6B 模型全量微调需要多少显存？

让我们把主要部分加起来（假设使用 FP16/BF16 精度训练，优化器状态使用 FP32）：

*   模型参数: 12 GB (FP16)
*   梯度: 12 GB (FP16)
*   优化器状态: 48 GB (FP32 for AdamW)
*   **基础显存需求 (不含激活值和杂项): 12 + 12 + 48 = 72 GB**

这仅仅是模型、梯度和优化器状态的基础开销！还没算上**非常可观**的激活值占用。即使使用了梯度检查点技术，激活值仍然会占用相当一部分显存（可能几 GB 到几十 GB，取决于配置）。

**因此，我们可以保守估计，全量微调一个 6B 参数的模型，即使使用混合精度训练和梯度检查点，通常也需要至少 80GB 以上的显存，如果 Batch Size 或序列长度稍大，需求轻松超过 100GB。**

这意味着什么？单张消费级显卡（如 RTX 3090/4090 的 24GB）是远远不够的。即使是专业级的 NVIDIA A100 (40GB 或 80GB) 或 H100 (80GB)，单卡运行全量微调也可能非常勉强或需要极小的 Batch Size 和序列长度，这会影响训练效率和效果。通常需要多卡并行（如使用 DeepSpeed ZeRO 等技术）才能进行。

这对于许多资源有限的开发者和研究者来说，无疑是一个巨大的门槛。

### 救星登场：参数高效微调 (PEFT)

面对如此高昂的显存（以及计算）成本，难道我们就束手无策了吗？当然不是！这就是**参数高效微调（Parameter-Efficient Fine-Tuning, PEFT）**技术大放异彩的地方。

**PEFT 的核心思想是：**

> **冻结（Freeze）** 预训练大模型的大部分（甚至几乎全部）参数，只微调其中**少量**或**额外增加**的一小部分参数。

这样做的好处是显而易见的：

1.  **大幅降低显存需求:**
    *   **梯度:** 只需为少量可训练参数计算和存储梯度。
    *   **优化器状态:** 只需为少量可训练参数存储优化器状态。
    *   模型参数本身（占大头）虽然加载了，但不产生梯度和优化器状态，极大地节省了显存。
    *   激活值部分依然存在，但由于可训练参数减少，整体显存压力骤减。
2.  **减少计算量，加快训练速度:** 反向传播和参数更新的计算量大大减少。
3.  **降低存储成本:** 每个任务只需要存储少量修改的参数，而不是整个模型的副本。
4.  **可能获得更好的性能:** 有研究表明，PEFT 方法有时能比全量微调在新任务上表现更好，尤其是在低数据场景下，可以有效防止**灾难性遗忘（Catastrophic Forgetting）**，并可能具有更好的泛化能力。

#### PEFT 技术概览

根据其实现方式，PEFT 技术大致可以分为三类：

1.  **增加额外参数 (Adding Extra Parameters, A):** 在原有模型结构中插入或附加少量新的、可训练的模块或参数，冻结原始参数。
    *   **类适配器 (Adapter-like) 方法:** 如 **Adapter Tuning**，在 Transformer 层之间插入小的瓶颈结构（Adapter 模块）。
    *   **软提示 (Soft Prompts):** 如 **Prefix Tuning**, **Prompt Tuning**, **P-Tuning** 等。它们不是调整模型权重，而是在输入或中间层添加可训练的连续向量（Virtual Tokens 或 Prompts），引导模型在下游任务上表现。
2.  **选取一部分参数更新 (Selecting Parameters, S):** 只选择模型中的一部分已有参数进行微调。
    *   例如 **BitFit**，它只微调模型中的 Bias（偏置）参数，或者只微调某几层。
3.  **引入重参数化 (Reparameterization, R):** 利用一些参数化的方式来表示参数的更新，从而减少需要直接优化的参数量。
    *   **LoRA** 也可以看作是重参数化的一种形式。

<img src="https://picx.zhimg.com/v2-eaaf1c00d0c4ea350cd3a79b47de26d3_r.jpg" alt="img" style="zoom:50%;" />

以上提到的 **BitFit、Prefix Tuning、Prompt Tuning、P-Tuning、Adapter Tuning、LoRA** 等都是当前非常热门且有效的 PEFT 方法。它们各自有不同的设计哲学和适用场景。

### 总结与展望

通过今天的分析，我们了解到：

*   全量微调一个 6B 参数的大模型需要巨大的显存（通常 > 80GB），主要是因为模型参数、梯度、优化器状态和激活值的累积占用。
*   高昂的显存成本使得全量微调对于许多人来说难以承受。
*   参数高效微调（PEFT）通过只微调少量参数，极大地降低了显存和计算需求，使得在有限资源下微调大模型成为可能。
*   PEFT 不仅节省资源，有时还能带来更好的性能和泛化能力。

理解了全量微调的显存瓶颈，就更能体会 PEFT 技术的价值所在。它为大模型的普及和应用落地打开了一扇重要的门。

在接下来的文章中，我将和大家深入探讨一些主流的 PEFT 方法，比如 LoRA、Prompt Tuning 等，讲解它们的原理、实现和使用技巧。敬请期待！

希望这篇博客能帮助你更好地理解大模型微调中的显存问题，并在你的学习或面试中有所帮助。如果你有任何问题或想法，欢迎在评论区留言交流！

---