# 老子今天就死磕激活函数

# 前言

**深度学习中的激活函数是人工神经网络中非常重要的组成部分**，它们**负责将神经元的输入映射到输出端。**激活函数在人工神经网络模型学习、理解复杂和非线性函数时起着关键作用，它们将非线性特性引入网络中。下面，我将详细讲解激活函数的定义、作用、常见的激活函数及其优缺点。

## 1、激活函数的定义与作用

激活函数（Activation Function）是在人工神经网络的神经元上运行的函数，它的主要作用是对神经元的输出进行缩放或转换，使其具备非线性特性。这种非线性特性对于神经网络来说至关重要，因为它能够帮助网络学习和表示复杂的数据模式。此外，激活函数还能增强网络关注的特征，减弱不关注的特征，从而优化网络的性能。

## 2、激活函数的性质

**非线性性**：激活函数引入了非线性，使得神经网络可以学习复杂的非线性关系。如果没有激活函数，多层神经网络将等效于单个线性层，无法捕捉到非线性特征。

**可微性**：激活函数通常要求是可微的，因为在反向传播算法中需要计算梯度来更新网络参数。绝大多数常见的激活函数都是可微的，但有些如ReLU，在零点不可导，但是可以在零点处约定一个导数值。

**单调性：**激活函数最好是单调的，这样可以保证损失函数是凸函数，使得优化问题更容易求解。大多数常用的激活函数都是单调的。

**输出范围**：不同的激活函数有不同的输出范围。有些激活函数的输出范围在0,1之间，适用于二元分类问题，比如sigmoid函数；有些输出范围在0, 之间，如ReLU函数，适用于回归问题；还有一些输出范围在[-1, 1]之间，如tanh函数，也适用于分类和回归问题。

**饱和性：**激活函数的饱和性指的是在某些输入范围内，函数的梯度很小，导致梯度消失问题。一些激活函数在输入很大或很小时会饱和，导致梯度接近于零，这会减缓学习速度或导致梯度消失。因此，一些激活函数被设计成在一定范围内不会饱和，如Leaky ReLU、ELU等。

**计算效率**：激活函数的计算效率也是一个考虑因素。一些激活函数的计算比较复杂，会增加训练和推理的时间成本，而有些激活函数计算较简单，如ReLU。

**稀疏性**：有些激活函数具有稀疏性，即在网络训练过程中，部分神经元的输出会趋向于零，这可以起到正则化的作用，有助于减少过拟合。

**归一化：**归一化的主要思想是使样本自动归一化到零均值、单位方差的分布，从而稳定训练，防止过拟合。



# 常见的激活函数（sigmoid系列）

## 2.1 Sigmoid函数：

### 1. 函数定义

Sigmoid函数是一种经典的**S型非线性激活函数**，其数学表达式为：
$$
\sigma(x) = \frac{1}{1 + e^{-x}}
$$
其导数可表示为：
$$
\sigma'(x) = \sigma(x)(1 - \sigma(x))
$$

- 将输入值映射到一个区间之间，实现非线性变换。
- 常用于二分类问题的输出层，将输出值转化为概率值。
- 在浅层网络中，用于处理二元分类任务。

### 2. 优点

- 具有很好的数学性质，具备平滑性和连续性。
- 输出范围在之间，可以被解释为概率。
- 相对简单，易于理解和实现。

### 3. 缺点

 **梯度消失严重**  

- 当$|x|>3$时梯度趋近于零（最大导数仅0.25），深层网络反向传播时参数更新困难  
- 饱和区的存在导致网络训练后期收敛缓慢

 **非零中心化**  

- 输出均值恒为正数，导致梯度更新呈锯齿状路径，降低收敛速度

 **计算资源消耗**  

- 涉及指数运算，相比ReLU等函数计算成本较高

### 4. Sigmoid函数实现及可视化图像

```
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def sigmoid_derivative(x):
    return sigmoid(x) * (1 - sigmoid(x))

# 生成一系列输入值
x_values = np.linspace(-10, 10, 100)

# 计算Sigmoid函数及其导数的值

y_values_sigmoid = sigmoid(x_values)
y_values_derivative = sigmoid_derivative(x_values)

# 可视化Sigmoid函数及其导数

plt.figure(figsize=(10, 6))
plt.plot(x_values, y_values_sigmoid, label='Sigmoid', color='blue')
plt.plot(x_values, y_values_derivative, label='Sigmoid Derivative', color='red', linestyle='--')
plt.title('Sigmoid Function and Its Derivative')
plt.xlabel('Input')
plt.ylabel('Output')
plt.grid(True)
plt.legend()
plt.show()
```

 函数(蓝线)及导数(红线)图像如下：

![img](https://i-blog.csdnimg.cn/blog_migrate/4c2e5f680b78acf619bb19e60c2b7bf2.png)

## 2.2 Tanh函数

### 函数定义

双曲正切函数定义为：
$$
\text{tanh}(x) = \frac{e^{x} - e^{-x}}{e^{x} + e^{-x}} = 2\sigma(2x) - 1
$$
其导函数具有简洁形式：
$$
\frac{d}{dx}\text{tanh}(x) = 1 - \text{tanh}^2(x)
$$

### 优点

- **非线性映射**：S型曲线增强网络非线性表达能力
- **对称特性**：输出范围(-1,1)，满足零均值分布（Zero-Centered）

### 缺点

1. **梯度衰减现象**：

   - 当|x|>2时，导数$1-\text{tanh}^2(x)$趋近于0

   - 深层网络反向传播存在衰减链式反应：
     $$
     \prod_{k=1}^n (1-\text{tanh}^2(x_k)) \approx 0
     $$

2. **计算复杂度**：

   - 单次计算包含4次指数运算（对比ReLU的1次判断）
   - GPU运算耗时比ReLU高

3. **参数敏感区**：

   - 有效工作区间仅限(-1.5,1.5)

### Tanh函数实现及可视化图像

```
def tanh(x):
    return (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))

def tanh_derivative(x):
    return 1 - np.square(tanh(x))

# 生成一系列输入值

x_values = np.linspace(-10, 10, 100)

# 计算Tanh函数及其导数的值

y_values_tanh = tanh(x_values)
y_values_derivative = tanh_derivative(x_values)

# 可视化Tanh函数及其导数

plt.figure(figsize=(10, 6))
plt.plot(x_values, y_values_tanh, label='Tanh', color='blue')
plt.plot(x_values, y_values_derivative, label='Tanh Derivative', color='red', linestyle='--')
plt.title('Tanh Function and Its Derivative')
plt.xlabel('Input')
plt.ylabel('Output')
plt.grid(True)
plt.legend()
plt.show()
```

 函数(蓝线)及导数(红线)图像如下：
![img](https://i-blog.csdnimg.cn/blog_migrate/d9b74f96013cc41b0c7d7b0aeabd902c.png)

## 2.3 Swish函数（silu）

### 1.函数定义

Swish函数是由Google提出的一种激活函数，它具有一种非线性的形式，并且在深度学习中表现良好。自门控激活函数定义为：
$$
\text{Swish}(x) = x \cdot \sigma(\beta x) = \frac{x}{1 + e^{-\beta x}}
$$
其梯度表达式为：
$$
\frac{d}{dx}\text{Swish}(x) = \sigma(\beta x) + \beta x \cdot \sigma(\beta x)(1 - \sigma(\beta x))
$$
当$\beta \to 0^+$时退化为线性函数，$\beta=1$时为标准Swish，即silu，$\beta$可学习时为Adaptive Swish。

Swish函数在输入较大时接近线性，而在输入较小时接近零。这使得它比一些传统的激活函数更具有优势，因为它可以将正向传播和反向传播的信号都保持在较大的范围内。

### 2.优点

- 非线性：Swish函数是一种非线性激活函数，可以帮助神经网络学习和表示复杂的非线性关系。
- 平滑性：Swish函数是平滑的，这有助于优化算法在训练神经网络时更容易地寻找最优解。
- 可学习的参数：Swish函数中的参数 β 可以通过反向传播进行优化，这使得神经网络可以自适应地调整参数。

### 3.缺点

- 计算成本：Swish函数的计算成本相对较高，因为它涉及到 sigmoid 函数的计算。
- 对比度敏感：Swish函数对比度敏感，可能会受到输入的大小和分布的影响。

###  4.函数图像及其可视化

```
def swish(x, beta=1.0):
    return x * (1 / (1 + np.exp(-beta * x)))

def swish_derivative(x, beta=1.0):
    sigmoid = 1 / (1 + np.exp(-beta * x))
    return (1 / (1 + np.exp(-beta * x))) + beta * x * sigmoid * (1 - sigmoid)

# 生成一系列输入值

x_values = np.linspace(-10, 10, 100)

# 计算Swish函数及其导数的值

y_values_swish = swish(x_values)
y_values_derivative = swish_derivative(x_values)

# 可视化Swish函数及其导数

plt.figure(figsize=(10, 6))
plt.plot(x_values, y_values_swish, label='Swish', color='blue')
plt.plot(x_values, y_values_derivative, label='Swish Derivative', color='red', linestyle='--')
plt.title('Swish Function and Its Derivative')
plt.xlabel('Input')
plt.ylabel('Output')
plt.grid(True)
plt.legend()
plt.show()
```

   函数(蓝线)及其导数(红线)图像如下：   

![img](https://i-blog.csdnimg.cn/blog_migrate/bfb20eaac446898a3382840e5b80e631.png)

## 2.4 Mish函数

### 1.函数定义

Mish函数，由Diganta Misra在2019年提出，它是对Swish函数的改进。Mish函数的数学表达式如下：
$$
\text{Mish}(x) = x \cdot \tanh\left( \text{Softplus}(x) \right) = x \cdot \tanh\left( \ln(1 + e^x) \right)
$$
Mish函数在负值区域呈现出类似于双曲正切函数的形状，而在正值区域则更加线性。它的形状在接近零点时比Swish函数更加平滑，因此在实践中有时候表现更好。

### 2.优点

- 非线性：Mish函数是一种非线性激活函数，有助于神经网络学习和表示复杂的非线性关系。
- 平滑性：Mish函数在接近零点时更加平滑，这有助于优化算法在训练神经网络时更容易地寻找最优解。

### 3.缺点

- 计算成本：Mish函数的计算成本相对较高，因为它涉及到tanh函数和指数函数的计算。
- 参数调整：Mish函数中没有可调参数，可能无法适应所有数据和问题的特性。

###   4.函数图像及其可视化

```
import numpy as np
import matplotlib.pyplot as plt

def mish(x):
    return x * np.tanh(np.log(1 + np.exp(x)))

def mish_derivative(x):
    sigmoid = 1 / (1 + np.exp(-x))
    return np.tanh(np.log(1 + np.exp(x))) + x * sigmoid * (1 - np.square(np.tanh(np.log(1 + np.exp(x)))))

# 生成一系列输入值

x_values = np.linspace(-10, 10, 100)

# 计算Mish函数及其导数的值

y_values_mish = mish(x_values)
y_values_derivative = mish_derivative(x_values)

# 可视化Mish函数及其导数

plt.figure(figsize=(10, 6))
plt.plot(x_values, y_values_mish, label='Mish', color='blue')
plt.plot(x_values, y_values_derivative, label='Mish Derivative', color='red', linestyle='--')
plt.title('Mish Function and Its Derivative')
plt.xlabel('Input')
plt.ylabel('Output')
plt.grid(True)
plt.legend()
plt.show()
```

  函数(蓝线)及其导数(红线)图像如下：
![img](https://i-blog.csdnimg.cn/blog_migrate/1df8cc135d21685b18a1d9d22c5ee8fd.png)

## 2.5 Softmax函数

### 1.函数定义

Softmax函数是一种常用的激活函数，通常用于多分类问题中的输出层。其数学表达式为：
$$
\text{Softmax}(\mathbf{x})_i = \frac{e^{x_i}}{\sum_{j=1}^N e^{x_j}} \quad \text{for } i=1,2,\dots,N
$$
其中：

- $\mathbf{x} = [x_1, x_2, \dots, x_N]$：输入向量；
- $\text{Softmax}(\mathbf{x})_i$：第$i$个类别的预测概率。

函数特性：

1. **概率归一化**：输出满足$0 \leq \text{Softmax}(\mathbf{x})_i \leq 1$且$\sum_{i=1}^N \text{Softmax}(\mathbf{x})_i = 1$；
2. **指数映射**：通过指数运算将原始分数$x_i$转换为正数，再通过归一化得到概率分布。

### 2.优点

- **输出概率分布：**Softmax函数将输入向量映射到一个概率分布上，使得每个元素的取值范围在0到1之间，并且所有元素的和为1。这种特性使得Softmax函数在多分类问题中特别有用，可以直接输出各个类别的概率。
- **数学简单：**Softmax函数的数学表达式相对简单，只涉及到指数运算和归一化操作，计算也比较高效。
- **可导性：**Softmax函数是可导的，这意味着可以直接应用于梯度下降等优化算法中，用于训练神经网络。

### 3.缺点

- **数值不稳定性**  
  当输入$x_i$较大时，$e^{x_i}$可能导致数值溢出。解决方法是引入**数值稳定技巧**：
  $$
  \text{StableSoftmax}(\mathbf{x})_i = \frac{e^{x_i - \max(\mathbf{x})}}{\sum_{j=1}^N e^{x_j - \max(\mathbf{x})}}
  $$
  通过减去最大值$\max(\mathbf{x})$，避免指数爆炸。

- **不适合处理大规模输出：**

  当输出类别数量较多时，Softmax 函数的计算量会增加，导致计算效率降低。特别是在深度学习模型中，如果输出类别数量非常大，Softmax 函数可能成为性能瓶颈。

  - 时间复杂度$O(n)$限制大规模分类
  - 需要存储所有指数运算中间结果（显存占用比ReLU高）

- **独立性假设：**

  Softmax函数假设各个输出之间相互独立，但在实际问题中可能存在相关性，这会导致Softmax函数的预测结果存在一定的偏差。

###   4.函数图像及其可视化

```
import numpy as np
import matplotlib.pyplot as plt

def softmax(z):
    exp_z = np.exp(z)
    return exp_z / np.sum(exp_z)

def softmax_derivative(z):
    softmax_output = softmax(z)
    return softmax_output * (1 - softmax_output)

# 生成输入范围

x_values = np.linspace(-10, 10, 100)

# 计算 Softmax 函数及其导数的输出

softmax_output = softmax(x_values)
softmax_derivative_output = softmax_derivative(x_values)

# 可视化 Softmax 函数及其导数

plt.figure(figsize=(10, 6))
plt.plot(x_values, softmax_output, label='Softmax', color='blue')
plt.plot(x_values, softmax_derivative_output, label='Softmax Derivative', color='red', linestyle='--')
plt.title('Softmax Function and Its Derivative')
plt.xlabel('Input')
plt.ylabel('Output')
plt.grid(True)
plt.legend()
plt.show()
```

  函数(蓝线)及其导数(红线)图像如下：   

![img](https://i-blog.csdnimg.cn/blog_migrate/812a5e7450f94fd3cb4c448fafd9a3f3.png)

# 常见的激活函数（relu系列）

## 2.6ReLU 函数 

### 函数定义

修正线性单元定义为：
$$
\text{ReLU}(x) = \max(0, x) = 
\begin{cases} 
x & \text{当 } x > 0 \\
0 & \text{当 } x \leq 0 
\end{cases}
$$
其分段导数为：
$$
\frac{d}{dx}\text{ReLU}(x) = 
\begin{cases} 
1 & \text{当 } x > 0 \\
0 & \text{当 } x \leq 0 
\end{cases}
$$

### 优点

1. **计算高效性**  
   仅需阈值判断（比较与取最大值操作），无需指数运算或复杂计算，显著降低计算复杂度，尤其适用于大规模数据和深度网络。

2. **稀疏激活特性**  
   当$x \leq 0$时输出为0，使得部分神经元处于非激活状态。这种稀疏性有助于：
   - **降低计算量**：未激活神经元的梯度为0，减少反向传播计算；
   - **提升泛化能力**：强制网络关注关键特征；
   - **参数稀疏性**：促进权重的稀疏分布，可能减少过拟合风险。

3. **缓解梯度消失问题**  
   在正区间（$x>0$）的导数恒为1，避免了Sigmoid/Tanh在饱和区导数趋近于0的问题，加速深层网络的训练收敛。

### 缺点

1. **Dead ReLU问题**  
   当输入$x \leq 0$时，导数为0，导致对应神经元的权重参数在反向传播中无法更新（即“死亡神经元”）。这可能由以下原因引发：
   - **初始化不当**：权重初始化过大会导致输入持续为负；
   - **学习率过高**：参数更新步长过大，使神经元进入不可恢复的非激活状态。

2. **输出非零中心化**  
   输出始终非负（$f(x) \geq 0$），导致梯度方向可能偏向单侧，影响优化效率。例如，在全连接层中，若所有神经元输出为正，权重更新可能集中在某一方向。

### ReLU函数图像及可视化

```
def relu(x):
    return np.maximum(0, x)

def relu_derivative(x):
    return np.where(x > 0, 1, 0)

# 生成一系列输入值

x_values = np.linspace(-10, 10, 100)

# 计算ReLU函数及其导数的值

y_values_relu = relu(x_values)
y_values_derivative = relu_derivative(x_values)

# 可视化ReLU函数及其导数

plt.figure(figsize=(10, 6))
plt.plot(x_values, y_values_relu, label='ReLU', color='blue')
plt.plot(x_values, y_values_derivative, label='ReLU Derivative', color='red', linestyle='--')
plt.title('ReLU Function and Its Derivative')
plt.xlabel('Input')
plt.ylabel('Output')
plt.grid(True)
plt.legend()
plt.show()
```

 函数(蓝线)及导数(红线)图像如下： 

![img](https://i-blog.csdnimg.cn/blog_migrate/1b2d4c76e71eee83e2ff7a74507f24f2.png)

## 2.7 Leaky ReLU函数

### 1.函数定义

改进型修正线性单元定义为：
$$
\text{LeakyReLU}(x) = 
\begin{cases} 
x & x > 0 \\
\alpha x & x \leq 0 
\end{cases}
\quad (\alpha \in (0,1))
$$
其导函数为分段常数：
$$
\frac{d}{dx}\text{LeakyReLU}(x) = 
\begin{cases} 
1 & x > 0 \\
\alpha & x \leq 0 
\end{cases}
$$

### 2.优点

1. **缓解"死亡神经元"问题**  
   在$x \leq 0$时，导数$f'(x)=\alpha$（而非0），使得神经元在负区间仍能接收梯度信号。这避免了ReLU中因导数为0导致的权重参数无法更新的问题。

2. **非线性建模能力**  
   函数在$x>0$时导数为1，$x \leq 0$时导数为$\alpha$，其分段线性特性仍能保持非线性映射能力，支持复杂模式学习。

3. **计算效率**  
   计算仅需阈值判断与线性运算（$\alpha x$），复杂度与ReLU相近，适用于大规模数据和深度网络。

### 3.缺点

1. **输出非零中心化**  
   尽管引入了负区间斜率，但输出仍为非对称分布。

2. **参数依赖性**  
   斜率参数$\alpha$需要人工设定，其最优值依赖具体任务（如$\alpha=0.01$是常见默认值，但缺乏理论指导）。不当选择可能导致：
   - $\alpha$过小时：接近ReLU的"死亡神经元"问题；
   - $\alpha$过大时：可能破坏输入的负区间分布特性。

### 4.函数图像及可视化

```
def leaky_relu(x, alpha=0.01):
    return np.where(x > 0, x, alpha * x)

def leaky_relu_derivative(x, alpha=0.01):
    return np.where(x > 0, 1, alpha)

# 生成一系列输入值

x_values = np.linspace(-10, 10, 100)

# 计算Leaky ReLU函数及其导数的值

y_values_leaky_relu = leaky_relu(x_values)
y_values_derivative = leaky_relu_derivative(x_values)

# 可视化Leaky ReLU函数及其导数

plt.figure(figsize=(10, 6))
plt.plot(x_values, y_values_leaky_relu, label='Leaky ReLU', color='blue')
plt.plot(x_values, y_values_derivative, label='Leaky ReLU Derivative', color='red', linestyle='--')
plt.title('Leaky ReLU Function and Its Derivative')
plt.xlabel('Input')
plt.ylabel('Output')
plt.grid(True)
plt.legend()
plt.show()
```

 函数(蓝线)及其导数(红线)图像如下：
![img](https://i-blog.csdnimg.cn/blog_migrate/36519bac3e829e9a03acc4c7ec08fe0e.png)

##  2.8 PReLU函数

### 1.函数定义

参数化修正线性单元定义为：
$$
\text{PReLU}(x) = 
\begin{cases} 
x & x > 0 \\
\alpha x & x \leq 0 
\end{cases}
\quad (\alpha \in \mathbb{R}^+)
$$
其参数梯度计算式为：
$$
\begin{aligned}
\frac{\partial f}{\partial x} &= \begin{cases} 
1 & x > 0 \\
\alpha & x \leq 0 
\end{cases} \\
\frac{\partial f}{\partial \alpha} &= \begin{cases} 
0 & x > 0 \\
x & x \leq 0 
\end{cases}
\end{aligned}
$$
其中$\alpha$通过反向传播自动学习，初始建议值$\alpha_0=0.25$。

```
torch.nn.PReLU(num_parameters=1, init=0.25, device=None, dtype=None)

输入：
num_parameters(整数)：可学习参数a的数量，只有两种选择，要么定义成1，表示在所有通道上应用相同的a进行激活，要么定义成输入数据的通道数，表示在所有通道上应用不同的a进行激活，默认1。

init(float)：a aa的初始值
```

```
一般用法

import torch.nn as nn
import torch

PReLU = nn.PReLU()
x = torch.randn(10)
value = PReLU(x)
print(x)
print(value)

# 输入
tensor([ 0.2399, -0.3208, -0.7234,  1.6305,  0.5196, -0.7686,  0.1195, -0.2320,
         1.2424, -0.7216])
# 激活值
tensor([ 0.2399, -0.0802, -0.1809,  1.6305,  0.5196, -0.1922,  0.1195, -0.0580,
         1.2424, -0.1804], grad_fn=<PreluBackward>)
```

```
有多个a时

import torch.nn as nn
import torch

PReLU = nn.PReLU(num_parameters=3, init=0.1)
x = torch.randn(12).reshape(4,3)
value = PReLU(x)
print(x)
print(value)
print(PReLU.weight)

# 输入

tensor([[-0.5554,  0.2285,  1.0417],
        [ 0.0180,  0.1619,  2.1579],
        [ 0.1636, -1.1147, -1.9901],
        [-0.4662,  1.5423,  0.0380]])

# 输出

tensor([[-0.0555,  0.2285,  1.0417],
        [ 0.0180,  0.1619,  2.1579],
        [ 0.1636, -0.1115, -0.1990],
        [-0.0466,  1.5423,  0.0380]], grad_fn=<PreluBackward>)

# 参数a

Parameter containing:
tensor([0.1000, 0.1000, 0.1000], requires_grad=True)

# 输入

tensor([ 0.2399, -0.3208, -0.7234,  1.6305,  0.5196, -0.7686,  0.1195, -0.2320,
         1.2424, -0.7216])

# 激活值

tensor([ 0.2399, -0.0802, -0.1809,  1.6305,  0.5196, -0.1922,  0.1195, -0.0580,
         1.2424, -0.1804], grad_fn=<PreluBackward>)
```



### 2.优点

- **自适应性**  
  参数$\alpha$通过反向传播动态调整，使函数能自适应不同数据分布和任务需求。

  每个神经元拥有独立可学习斜率参数$\alpha_i$（也可以层共享）

- **缓解梯度消失问题**  
  在负区间（$x \leq 0$）的导数为$\alpha$（而非0），确保梯度信号持续传递。这避免了ReLU中因导数为0导致的"死亡神经元"问题。

- **保持稀疏性**  
  尽管引入了负区间斜率，但$\alpha$通常取较小值（如0.01~0.25），使得大部分负输入的输出仍接近0。

### 3.缺点

1️⃣ **计算复杂度增加**  

- 每个通道需存储$\alpha$参数
- 反向传播需计算$\frac{\partial L}{\partial \alpha}$，训练时间增加

2️⃣ **优化难度提升**  

- 需配合自适应优化器（如Adam）防止参数震荡
- 小数据集易出现$\alpha$过拟合（需搭配L2正则）

### 4.函数图像及可视化

```
class PRelu:
    def __init__(self, alpha):
        self.alpha = alpha
        

    def activate(self, x):
        return np.where(x > 0, x, self.alpha * x)
    
    def gradient(self, x):
        return np.where(x > 0, 1, self.alpha)

# 创建 PRelu 实例

prelu = PRelu(alpha=0.1)

# 生成一系列输入值

x_values = np.linspace(-5, 5, 100)

# 计算 PRelu 函数及其导数的值

y_values_prelu = prelu.activate(x_values)
y_values_derivative = prelu.gradient(x_values)

# 可视化 PRelu 函数及其导数

plt.figure(figsize=(10, 6))
plt.plot(x_values, y_values_prelu, label='PRelu', color='blue')
plt.plot(x_values, y_values_derivative, label='PRelu Derivative', color='red', linestyle='--')
plt.title('PRelu Function and Its Derivative')
plt.xlabel('Input')
plt.ylabel('Output')
plt.grid(True)
plt.legend()
plt.show()
```

  函数(蓝线)及其导数(红线)图像如下： 
![img](https://i-blog.csdnimg.cn/blog_migrate/75cd5cd5c0bd082bc1879972a59784c4.png)

## 2.9 ELU函数

### 1.函数定义

指数线性单元定义为：
$$
\text{ELU}(x) = 
\begin{cases} 
x & x > 0 \\
\alpha(e^x - 1) & x \leq 0 
\end{cases}
\quad (\alpha \geq 0)
$$
其导函数表达式为：
$$
\frac{d}{dx}\text{ELU}(x) = 
\begin{cases} 
1 & x > 0 \\
\text{ELU}(x) + \alpha & x \leq 0 
\end{cases}
$$

### 2.优点

**缓解"死亡神经元"问题**  

- 在负区间（$x \leq 0$）的导数为$\frac{d}{dx}f(x) = \alpha e^x$，始终大于0。这确保了即使输入为负，梯度信号仍能传递，避免了ReLU中导数为0导致的权重更新停滞问题。

 **零中心化特性**  

- 输出均值趋近于零（$\mu_{out} \approx 0$当$\alpha=1$），加速参数收敛

### 3.缺点

1️⃣ **计算复杂度陡增**  

- 引入指数运算使单次前向计算耗时增加（对比ReLU）

2️⃣ **超参数敏感性**  

- $\alpha$值设置影响模型鲁棒性（推荐范围0.1-1.0）



### 4.函数图像及其可视化

```
def elu(x, alpha=1.0):
    return np.where(x > 0, x, alpha * (np.exp(x) - 1))

def elu_derivative(x, alpha=1.0):
    return np.where(x > 0, 1, alpha * np.exp(x))

# 生成一系列输入值

x_values = np.linspace(-10, 10, 100)

# 计算ELU函数及其导数的值

y_values_elu = elu(x_values)
y_values_derivative = elu_derivative(x_values)

# 可视化ELU函数及其导数

plt.figure(figsize=(10, 6))
plt.plot(x_values, y_values_elu, label='ELU', color='blue')
plt.plot(x_values, y_values_derivative, label='ELU Derivative', color='red', linestyle='--')
plt.title('ELU Function and Its Derivative')
plt.xlabel('Input')
plt.ylabel('Output')
plt.grid(True)
plt.legend()
plt.show()
```

  函数(蓝线)及其导数(红线)图像如下：  

![img](https://i-blog.csdnimg.cn/blog_migrate/648151807f1d15c598908fe3dd854452.png)

## 2.10 SELU函数

### 1.函数定义

SELU（Scaled Exponential Linear Unit）函数是一种激活函数，是对ELU函数的扩展和缩放版本。缩放指数线性单元定义为：
$$
\text{SELU}(x) = \lambda \cdot 
\begin{cases} 
x & x > 0 \\
\alpha(e^x - 1) & x \leq 0 
\end{cases}
$$
其中参数通常满足：
$$
\lambda=1.0507, \quad \alpha=1.67326
$$
这些参数通过求解积分方程得出：
$$
\int_{-\infty}^{\infty} \text{SELU}(x)\mathcal{N}(0,1)dx = 0 \\
\int_{-\infty}^{\infty} \text{SELU}^2(x)\mathcal{N}(0,1)dx = 1
$$
这些常数是根据自归一化条件推导出来的，使得网络在深度方向上保持稳定分布。

### 2.优点

1. **自归一化特性**  
   在满足以下条件时，SELU能保持输入的均值和方差在深度方向上稳定：
   - 输入服从独立同分布（i.i.d.）；
   - 线性变换为单位矩阵（权重初始化为$I$）；
   - 网络层间无批量归一化（Batch Normalization）。
     这一特性有效缓解了梯度消失/爆炸问题，尤其适用于深度网络。

2. **零中心特性**  
   输出范围为$[-\lambda \alpha, +\infty)$，负区间输出可为负值，使整体分布以零为中心。这有助于梯度方向的平衡性，提升优化效率。

3. **平滑性与连续可导性**  
   函数在$x=0$处连续且导数连续（$\lim_{x\to0^-} f'(x)=\lambda \alpha$，$\lim_{x\to0^+} f'(x)=\lambda$），避免了ReLU的不可导尖点问题，有利于梯度优化。

### 3.缺点

1. **严格的自归一化条件**  
   SELU的稳定性依赖于特定的网络结构约束（如权重矩阵为单位矩阵），这在实际中难以完全满足。例如：
   - 需要禁用批量归一化；
   - 权重初始化需严格遵循单位矩阵要求；
   - 输入分布需严格符合i.i.d.假设。

2. **计算复杂度**  
   负区间涉及指数运算$e^x$，其计算复杂度高于ReLU/PReLU的线性运算。尽管现代GPU可部分加速，但在资源受限场景中仍需权衡。

### 4.函数图像及其可视化

```
def selu(x, alpha=1.6733, scale=1.0507):
    return scale * np.where(x > 0, x, alpha * (np.exp(x) - 1))

def selu_derivative(x, alpha=1.6733, scale=1.0507):
    return scale * np.where(x > 0, 1, alpha * np.exp(x))

# 生成一系列输入值

x_values = np.linspace(-3, 3, 100)

# 计算SELU函数及其导数的值

y_values_selu = selu(x_values)
y_values_derivative = selu_derivative(x_values)

# 可视化SELU函数及其导数

plt.figure(figsize=(10, 6))
plt.plot(x_values, y_values_selu, label='SELU', color='blue')
plt.plot(x_values, y_values_derivative, label='SELU Derivative', color='red', linestyle='--')
plt.title('SELU Function and Its Derivative')
plt.xlabel('Input')
plt.ylabel('Output')
plt.grid(True)
plt.legend()
plt.show()
```

  函数(蓝线)及其导数(红线)图像如下：
![img](https://i-blog.csdnimg.cn/blog_migrate/33b33f788f31bf630c8985a346c78267.png)

## 2.11 GELU函数

### 1.函数定义

GELU（Gaussian Error Linear Unit）最初由Hendrycks和Gimpel在2016年提出，并且在BERT模型中广泛应用。GELU 函数的数学表达式如下：
$$
\text{GELU}(x) = x \cdot \Phi(x) = x \cdot \frac{1}{2}\left[1 + \text{erf}\left(\frac{x}{\sqrt{2}}\right)\right]
$$
其中$\Phi(x)$为标准正态分布的累积分布函数，$\text{erf}(x)$是误差函数：
$$
\text{erf}(x) = \frac{2}{\sqrt{\pi}} \int_{0}^x e^{-t^2} dt
$$

---

工程实现中常采用近似公式加速计算：
$$
\text{GELU}(x) \approx 0.5x\left(1+\tanh\left(\sqrt{\frac{2}{\pi}}(x + 0.044715x^3)\right)\right) \quad
$$

### 2.优点

**非线性性质：**

GELU 函数是一种非线性激活函数，可以帮助神经网络学习和表示复杂的非线性关系。

**平滑性：**

GELU 函数是平滑的，这有助于优化算法在训练神经网络时更容易地寻找最优解。

**性能表现：**

GELU 函数在很多自然语言处理任务中表现良好，尤其在BERT等模型中得到广泛应用。

### 3.缺点

1. **计算成本较高**  
   误差函数$\text{erf}(x)$的计算涉及指数运算和积分近似，复杂度为$O(1)$但常数因子较大。解决方案包括：
   - 使用近似公式（如上式）替代积分计算；
   - 在GPU上利用向量化运算加速。

2. **对比度敏感性**  
   GELU的输出与输入分布强相关。例如，当输入均值偏移或方差变化时，输出分布可能不稳定。建议在输入层添加**Normalization**以缓解此问题。

3. **无参数可调性**  
   GELU的形状固定，无法自适应不同数据分布。对比Leaky ReLU或PReLU，其灵活性较低。可通过以下方式弥补：
   - 结合标签平滑（Label Smoothing）提升泛化；
   - 在特定层使用可学习激活函数（如Swish）。

###   4.函数图像及其可视化

```
def gelu(x):
    return 0.5 * x * (1 + erf(x / np.sqrt(2)))

def gelu_derivative(x):
    return 0.5 * (1 + erf(x / np.sqrt(2))) + 0.5 * x * np.exp(-0.5 * x**2) / np.sqrt(np.pi)

# 生成一系列输入值

x_values = np.linspace(-5, 5, 100)

# 计算 GELU 函数及其导数的值

y_values_gelu = gelu(x_values)
y_values_derivative = gelu_derivative(x_values)

# 可视化 GELU 函数及其导数

plt.figure(figsize=(10, 6))
plt.plot(x_values, y_values_gelu, label='GELU', color='blue')
plt.plot(x_values, y_values_derivative, label='GELU Derivative', color='red', linestyle='--')
plt.title('GELU Function and Its Derivative')
plt.xlabel('Input')
plt.ylabel('Output')
plt.grid(True)
plt.legend()
plt.show()
```

  函数(蓝线)及其导数(红线)图像如下：    
![img](https://i-blog.csdnimg.cn/blog_migrate/49b45eaba8d4ceef394a00238f36703f.png)

## 2.12 Maxout函数

### 1.函数定义

Maxout是一种激活函数，由Goodfellow等人于2013年提出。它不是单个函数，而是一种整流单元（Rectified Linear Unit，ReLU）的扩展，能够学习激活函数的形状。

参数化整流单元定义为：
$$
\text{Maxout}(\mathbf{x}) = \max_{k \in [1,K]} (\mathbf{w}_k^T \mathbf{x} + b_k)
$$
其梯度计算规则为：
$$
\frac{\partial}{\partial \mathbf{x}} \text{Maxout}(\mathbf{x}) = \mathbf{w}_{k^*},\quad k^* = \arg\max_k (\mathbf{w}_k^T \mathbf{x} + b_k)
$$
其中$K$为整流单元数，默认配置$K=2$时即为原始论文设定。

### 2.优点

- **非线性性质**：Maxout 函数是一种非线性激活函数，可以帮助神经网络学习和表示复杂的非线性关系。

- **表达能力**：Maxout 函数的表达能力比 ReLU 更强，它能够学习到更复杂的函数形状，从而提高模型的拟合能力。

- **稀疏性**：与 ReLU 类似，Maxout 函数在某些情况下可以保持输入的稀疏性，因为在输入的某些部分会被置零。

### 3.缺点

- **参数量激增**  
  每个Maxout单元需要两倍于ReLU的参数（权重和偏置），导致模型复杂度显著增加。例如，若网络中使用$N$个Maxout单元，则参数量为$2N(d+1)$（$d$为输入维度），可能引发过拟合，尤其在小数据集上。

- **计算与存储成本**  
  额外的线性组合运算增加了前向传播和反向传播的计算量。在GPU上，虽然并行计算可部分缓解，但参数存储开销仍可能成为瓶颈。

###   4.函数图像及其可视化

```
def maxout(x, w1, b1, w2, b2):
    max1 = np.dot(x, w1) + b1
    max2 = np.dot(x, w2) + b2
    return np.maximum(max1, max2)

def maxout_derivative(x, w1, b1, w2, b2):
    max1 = np.dot(x, w1) + b1
    max2 = np.dot(x, w2) + b2
    return np.where(max1 > max2, w1, w2)

# 生成一系列输入值

x_values = np.linspace(-5, 5, 100)

# 设置权重和偏置

w1 = np.array([1, 1])
b1 = 0
w2 = np.array([-1, 1])
b2 = 0

# 计算 Maxout 函数及其导数的值

y_values_maxout = maxout(x_values[:, np.newaxis], w1[np.newaxis, :], b1, w2[np.newaxis, :], b2)
y_values_derivative = maxout_derivative(x_values[:, np.newaxis], w1[np.newaxis, :], b1, w2[np.newaxis, :], b2)

# 可视化 Maxout 函数及其导数

plt.figure(figsize=(10, 6))
plt.plot(x_values, y_values_maxout, label='Maxout', color='blue')
plt.plot(x_values, y_values_derivative, label='Maxout Derivative', color='red', linestyle='--')
plt.title('Maxout Function and Its Derivative')
plt.xlabel('Input')
plt.ylabel('Output')
plt.grid(True)
plt.legend()
plt.show()
```

   函数(蓝线)及其导数(红线)图像如下：  

![img](https://i-blog.csdnimg.cn/blog_migrate/5820b0a7eb1807f1218ac50a0b2182b0.png)

# 总结

神经网络中的激活函数是引入非线性和提高网络表达能力的重要组成部分。本文详细探讨了激活函数的性质、种类、及其优缺点。通过对Sigmoid、Tanh、ReLU等常见激活函数的深入分析，揭示了它们在神经网络中的关键作用。此外，针对梯度消失和“死亡”ReLU等挑战，给出了有效的解决方案。

综上所述，激活函数对于神经网络的性能和学习能力具有显著影响，选择合适的激活函数对于优化网络训练和提高模型性能至关重要。







————————————————

原文链接：https://blog.csdn.net/a910247/article/details/137573481