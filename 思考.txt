1. transformer或者说以transformer为基本架构的大语言模型各个部分的计算量与所需资源的情况如何分配的？

2. deepseek V3还是r1记不清了，应该是提出MHLA的那篇，为什么能够降低训练的成本？可以降低部署的成本可以理解，
因为降低了KVcahe，但是为什么可以降低训练成本？

3. rope细节

4. 为什么rope无法和MLA兼容

5. MLA能降低显存能理解，但是咋还能加速呢？是推理加速还是推理和训练都加速？